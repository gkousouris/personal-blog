---
title: 'Nrtsearch Tutorial - Indexing Web Content for Search'
date: '2023-04-12'
spoiler: Build a simple web crawler and website indexer. Support internal search with custom scoring for any website with nrtsearch.
blogImages: []
---

Let's use [nrtsearch](https://github.com/Yelp/nrtsearch) - an open-source search engine built by Yelp - to support text search for any website. In this tutorial, I'm using my blog as an example dataset, but you can use this approach to index anything accessible on the web, like Wikipedia or an internal company wiki.

We will begin by building a simple web crawler to fetch your website's data. Then we will start a local, standalone nrtsearch instance, design its index schema and set up an indexing pipeline to feed the data into the search engine. Finally, we will define nrtsearch queries to support custom scoring and highlighting. We will be able to interact with the system via a search UI. All of this will be done with minimal coding, I will make use of open-source projects to keep things simple and extensible You can clone [the tutorial repo](https://github.com/jedrazb/nrtsearch-tutorial-website-search) to run the code for each step locally. All you need is python3 and docker installed on your system.

If you want to learn more about nrtSearch you can read Yelp's Engineering blog post: [Nrtsearch: Yelp’s Fast, Scalable and Cost Effective Search Engine](https://engineeringblog.yelp.com/2021/09/nrtsearch-yelps-fast-scalable-and-cost-effective-search-engine.html). Let's start!

## Web crawler

The purpose of the crawler is to fetch the data from websites. In this tutorial I'm using my blog as an example dataset. For simplicity, I will supply a sitemap - a list of urls - to the crawler so that it can directly fetch the data.

Let's use [beautifulsoup](https://beautiful-soup-4.readthedocs.io/en/latest/) library for extracting the website content. The website data can be represented in a following way:

- `title` - website title tag
- `description` - meta description tag, a short summary of the website, provided by the author
- `url`
- `content`- extracted text content, e.g. paragraphs
- `headings` - extracted website's headings. For simplicity we group `h1`, `h2`, `h3`, ... tags together. This multi-valued field can be used for document scoring.

Our code uses `get_source_urls()` function to get the list of urls. The function will process the HTML pages to extract: title, description, content and headings. After it's done, it will save the output to a JSON file.

```python
import json
import requests
from bs4 import BeautifulSoup

urls = get_source_urls() # urls from website sitemap

website_data = []

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    title = soup.title.text.strip() if soup.title else None
    description = (
        soup.find("meta", attrs={"name": "description"})["content"].strip()
        if soup.find("meta", attrs={"name": "description"})
        else None
    )
    headings = [
        h.text.strip() for h in soup.find_all(["h1", "h2", "h3", "h4", "h5"])
    ]
    content = soup.get_text(" ", strip=True)

    website_data.append(
        {
            "url": url,
            "title": title,
            "description": description,
            "headings": headings,
            "content": content,
        }
    )

with open('website_data.json', 'w') as outfile:
    json.dump(website_data, outfile)

```

The full crawler code is located in [crawler/crawler.py](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/crawler/crawler.py), you can run the crawler by executing:

```bash
make run_crawler
```

## Starting the nrtsearch cluster

The nrtsearch cluster consists 2 types of nodes:

- `primary` - a single node, responsible for data indexing, it periodically publishes Lucene segment updates to replicas. Hence the name: _nrtsearch - near-real time search_.
- `replica` - one or more nodes, responsible for serving the search traffic. It receives periodic segment updates from the primary. The number of running replicas can be controlled by an auto-scaler (like [HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)).

It's also possible to run the cluster with a single, standalone node, used for both data ingestion and search. It's ok for experimentation but for production environments its recommended to run separate primary and replica nodes.

The source code for nrtsearch setup is available in [/nrtsearch](https://github.com/jedrazb/nrtsearch-tutorial-website-search/tree/master/nrtsearch) folder. There we have:

- `Dockerfile` to setup an nrtsearch node
- configs for primary and replicas
- `docker-compose.yaml` to setup a local cluster with single primary and two replicas

Note, in this tutorial we don't care about persisting the index state. If you restart the cluster all the state will be lost. In order to persist your index, you can attach a permanent volume. For production, it's best to use a dedicated s3 bucket where nrtsearch will preserve the cluster state. You can define it in the server configuration file, more about it in the [docs](https://nrtsearch.readthedocs.io/en/latest/server_configuration.html).

In order to start the local cluster with a primary and two replicas, run a following command in a separate terminal window:

```bash
make start_nrtsearch_cluster
```

When running `docker compose ps`, you should see:

```bash
NAME                       IMAGE                    COMMAND                  SERVICE             CREATED             STATUS              PORTS
nrtsearch-replica-node-1   nrtsearch-replica-node   "./entrypoint_replic…"   replica-node        6 seconds ago       Up 4 seconds        8000/tcp
nrtsearch-replica-node-2   nrtsearch-replica-node   "./entrypoint_replic…"   replica-node        6 seconds ago       Up 4 seconds        8000/tcp
primary-node               nrtsearch-primary-node   "bash -c '/user/nrts…"   primary-node        6 seconds ago       Up 4 seconds        8000/tcp

```

Great! Now we have a local nrtsearch cluster with nodes dedicated for data ingestion and search. Now, let's design the index schema and start the index.

## Index schema

## Creating the nrtsearch index

## nrtSearch indexer

## Custom search scoring

## Highlighting support

## Simple search UI

## Putting it all together

## Demo
