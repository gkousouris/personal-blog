---
title: 'Nrtsearch Tutorial - Indexing Web Content for Search'
date: '2023-04-12'
spoiler: Build a simple web crawler and website indexer. Support internal search with custom scoring for any website with nrtsearch.
blogImages: []
---

Let's use [nrtsearch](https://github.com/Yelp/nrtsearch) - an open-source search engine built by Yelp - to support text search for any website. In this tutorial, I'm using my blog as an example dataset, but you can use this approach to index any text content on the internet.

We will begin by building a simple web crawler to fetch your website's data. Then we will start a local, standalone nrtsearch instance, design its index schema and set up an indexing pipeline to feed the data into the search engine. Finally, we will define nrtsearch queries to support custom scoring and highlighting. We will be able to interact with the system via a search UI. All of this will be done with minimal coding, I will make use of open-source projects to keep things simple and extensible You can clone [the tutorial repo](https://github.com/jedrazb/nrtsearch-tutorial-website-search) to run the code for each step locally. All you need is `python3` and `docker` installed on your system.

If you want to learn more about nrtSearch you can read Yelp's Engineering blog post: [Nrtsearch: Yelp’s Fast, Scalable and Cost Effective Search Engine](https://engineeringblog.yelp.com/2021/09/nrtsearch-yelps-fast-scalable-and-cost-effective-search-engine.html). Let's start!

## Web crawler

The purpose of the crawler is to fetch the data from websites. In this tutorial I'm using my blog as an example dataset. For simplicity, I will supply a sitemap - a list of urls - to the crawler so that it can directly fetch the data.

Let's use [beautifulsoup](https://beautiful-soup-4.readthedocs.io/en/latest/) library for extracting the website content. The website data can be represented in a following way:

- `title` - website title tag
- `description` - meta description tag, a short summary of the website, provided by the author
- `url`
- `content`- extracted text content, e.g. paragraphs
- `headings` - extracted website's headings. For simplicity we group `h1`, `h2`, `h3`, ... tags together. This multi-valued field can be used for document scoring.

Our code uses `get_source_urls()` function to get the list of urls. The function will process the HTML pages to extract: title, description, content and headings. After it's done, it will save the output to a JSON file.

```python
import json
import requests
from bs4 import BeautifulSoup

urls = get_source_urls() # urls from website sitemap

website_data = []

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    title = soup.title.text.strip() if soup.title else None
    description = (
        soup.find("meta", attrs={"name": "description"})["content"].strip()
        if soup.find("meta", attrs={"name": "description"})
        else None
    )
    headings = [
        h.text.strip() for h in soup.find_all(["h1", "h2", "h3", "h4", "h5"])
    ]
    content = soup.get_text(" ", strip=True)

    website_data.append(
        {
            "url": url,
            "title": title,
            "description": description,
            "headings": headings,
            "content": content,
        }
    )

with open('website_data.json', 'w') as outfile:
    json.dump(website_data, outfile)

```

The full crawler code is located in [crawler/crawler.py](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/crawler/crawler.py), you can run the crawler by executing:

```bash
make run_crawler
```

## Starting the nrtsearch cluster

The nrtsearch cluster consists 2 types of nodes:

- `primary` - a single node, responsible for data indexing, it periodically publishes Lucene segment updates to replicas. Hence the name: _nrtsearch - near-real time search_.
- `replica` - one or more nodes, responsible for serving the search traffic. It receives periodic segment updates from the primary. The number of running replicas can be controlled by an auto-scaler (like [HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)).

It's also possible to run the cluster with a single, standalone node, used for both data ingestion and search. It's ok for experimentation but for production environments it's recommended to run separate primary and replica nodes.

The source code for nrtsearch setup is available in [/nrtsearch](https://github.com/jedrazb/nrtsearch-tutorial-website-search/tree/master/nrtsearch) folder. There we have:

- `Dockerfile` to setup an nrtsearch node
- configs for primary and replicas
- `docker-compose.yaml` to setup a local cluster with single primary and two replicas

Note, in this tutorial we don't care about persisting the index state. If you restart the cluster all the state will be lost. In order to persist your index, you can attach a permanent volume. For production, it's best to use a dedicated s3 bucket where nrtsearch will preserve the cluster state. This can be setup in configuration file, more about it in the [docs](https://nrtsearch.readthedocs.io/en/latest/server_configuration.html).

In order to start the local cluster with a primary and two replicas, run a following command in a separate terminal window:

```bash
make start_nrtsearch_cluster
```

Above command will run `docker compose up` that will take the primary/replica configuration from [docker-compose.yaml](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/nrtsearch/docker-compose.yaml) file. In your terminal window you should see logs indicating that the primary and replicas are ready, and the server has started:

```bash
primary-node              | [INFO ] 2023-04-11 08:32:16.712 [main] LuceneServer - Server started, listening on 8000 for messages
```

When running `docker compose ps`, you should see:

```bash
NAME                       IMAGE                    COMMAND                  SERVICE             CREATED             STATUS              PORTS
nrtsearch-replica-node-1   nrtsearch-replica-node   "./entrypoint_replic…"   replica-node        6 seconds ago       Up 4 seconds        8000/tcp
nrtsearch-replica-node-2   nrtsearch-replica-node   "./entrypoint_replic…"   replica-node        6 seconds ago       Up 4 seconds        8000/tcp
primary-node               nrtsearch-primary-node   "bash -c '/user/nrts…"   primary-node        6 seconds ago       Up 4 seconds        8000/tcp

```

Great! Now we have a local nrtsearch cluster with nodes dedicated for data ingestion and search. Now, let's design the index schema and start the index.

## Generating gRPC client code using protoc

Clients can communicate with nrtsearch via gRPC or REST API. gRPC is a high-performance remote procedure call framework that enables communication by using protocol buffers for serialization and deserialization of messages. This method yields better performance for production applications. The REST API is an optional way to communicate with nrtsearch, good for local experimentation. The REST server ([grpc-gateway](https://github.com/grpc-ecosystem/grpc-gateway)) is autogenerated from `proto` definitions.

Let's use gRPC to communicate with nrtsearch. Since it's not a REST API we can't just `curl` the endpoints. We need to generate the native client code, for this tutorial - `python`, to interact with nrtsearch.

To generate Python client code from proto definitions, you can use the gRPC toolchain's `protoc` compiler along with the `grpc_tools` package to compile the `.proto` files into Python code, and then use the generated client code to communicate with the gRPC server. The steps are:

- get all `.proto` along with their dependencies to a folder, let's call it `/protos`
- point `protoc` compiler to `/protos` folder and generate the python code

In the [Makefile](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/Makefile) there commands `nrtsearch_protos` and `nrtsearch_protos` which will execute above steps above and put the generated code into [/nrtsearch_client/nrtsearch_py_grpc](https://github.com/jedrazb/nrtsearch-tutorial-website-search/tree/master/nrtsearch_client/nrtsearch_py_grpc).

```Makefile
# Generate nrtsearch .proto files and their dependencies
nrtsearch_protos:
	mkdir -p protos
	docker build -t nrtsearch-protos-builder:latest ./utils/nrtsearch_protos_builder/
	docker run -v $(shell pwd)/protos:/user/protos  nrtsearch-protos-builder:latest

# Compile client .proto files to python code
nrtsearch_protoc: nrtsearch_protos
	mkdir -p protoc
	$(PYTHON) -m grpc_tools.protoc \
		--proto_path protos \
		--grpc_python_out protoc \
		--python_out protoc \
		protos/yelp/nrtsearch/luceneserver.proto \
		protos/yelp/nrtsearch/search.proto \
		protos/yelp/nrtsearch/analysis.proto \
		protos/yelp/nrtsearch/suggest.proto
	cp `find protoc -name "*.py"` nrtsearch_client/nrtsearch_py_grpc
	rm -rf protos protoc
```

In order to nrtsearch `.proto` files and their dependencies we use simple [Dockerfile](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/utils/nrtsearch_protos_builder/Dockerfile) that compiles the project and moves all proto files into the persistent volume `/protos`. Then protoc compiler compiles them to python code.

```bash
ls nrtsearch_client/nrtsearch_py_grpc/
analysis_pb2.py          luceneserver_pb2.py      search_pb2.py            suggest_pb2.py
analysis_pb2_grpc.py     luceneserver_pb2_grpc.py search_pb2_grpc.py       suggest_pb2_grpc.py
```

## Index schema

## Creating the nrtsearch index

## nrtSearch indexer

## Custom search scoring

## Highlighting support

## Simple search UI

## Putting it all together

## Demo
